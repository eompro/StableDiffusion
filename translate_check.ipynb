{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade diffusers[torch]\n",
    "!pip install transformers[sentencepiece]\n",
    "\n",
    "import torch\n",
    "\n",
    "from diffusers.models import AutoencoderKL, UNet2DConditionModel\n",
    "from diffusers.pipelines.stable_diffusion.safety_checker import StableDiffusionSafetyChecker\n",
    "from diffusers.schedulers import DDIMScheduler\n",
    "from transformers import (\n",
    "    CLIPFeatureExtractor,\n",
    "    CLIPTextModel,\n",
    "    CLIPTokenizer,\n",
    "    MBart50TokenizerFast,\n",
    "    MBartForConditionalGeneration,\n",
    "    pipeline,\n",
    ")\n",
    "\n",
    "from translate import KoreanStableDiffusion\n",
    "\n",
    "# Pretrain model & tokenizer all load\n",
    "\n",
    "MBart_model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50-many-to-one-mmt\", torch_dtype = torch.float16).to(\"cuda\")\n",
    "MBart_tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-one-mmt\")\n",
    "clip_tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype = torch.float16)\n",
    "clip_text_model = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype = torch.float16).to(\"cuda\")\n",
    "clip_feature_extractor = CLIPFeatureExtractor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "unet_model = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\", torch_dtype=torch.float16).to(\"cuda\")\n",
    "ddim_scheduler = DDIMScheduler.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"scheduler\", torch_dtype = torch.float16)\n",
    "auto_encoder_kl = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder = \"vae\", torch_dtype=torch.float16).to(\"cuda\")\n",
    "stable_diffusion_safety_checker = StableDiffusionSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder = \"safety_checker\", torch_dtype=torch.float16).to(\"cuda\")\n",
    "\n",
    "pipeline = KoreanStableDiffusion(\n",
    "      translation_model= MBart_model,\n",
    "      translation_tokenizer= MBart_tokenizer,\n",
    "      vae= auto_encoder_kl,\n",
    "      text_encoder= clip_text_model,\n",
    "      tokenizer= clip_tokenizer,\n",
    "      unet= unet_model,\n",
    "      scheduler= ddim_scheduler,\n",
    "      safety_checker= stable_diffusion_safety_checker,\n",
    "      feature_extractor= clip_feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pipeline(prompt = \"작고 귀여운 소 장난감\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.images[0]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
